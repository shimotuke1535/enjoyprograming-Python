# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JiAh_qDzd3wvJ2PAuBaWsSWs6r3SYovI

## 21C1062 下野真生
# MNISTを用いたニューラルネットワーク実習

### Googleドライブをマウントする
"""
import sys, os
sys.path.append(os.pardir) # 親ディレクトリのファイルをインポートするための設定
from dataset.mnist import load_mnist

"""### 手書き文字認識"""

(x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False)

print(x_train.shape)
print(t_train.shape)
print(x_test.shape)
print(t_test.shape)

import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist
from PIL import Image
from IPython.display import display

def img_show(img):
  pil_img = Image.fromarray(np.uint8(img))
  display(pil_img)

(x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False)

img = x_train[123]
label = t_train[123]

print("表示される数字のラベル：", label)
print("整形前のデータ形式：", img.shape)
img = img.reshape(28, 28)
print("整形後のデータ形式：",img.shape)

img_show(img)

# coding: utf-8
import sys, os
sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定
import numpy as np
import pickle
from dataset.mnist import load_mnist
from common.functions import *

def get_data():
    # データセットからテストデータを読み込む
    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return x_test, t_test

def init_network():
    # 学習済みのサンプルデータを読み込む
    with open("ch03/sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
    return network

def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)

    return y

x, t = get_data() # MNISTデータを取得（x:画像データ，t:教師データ）
network = init_network() # ネットワークを生成
accuracy_cnt = 0
for i in range(len(x)): # 全ての画像データをfor文で処理にかける
    y = predict(network, x[i]) # ニューラルネットワークを通して予測した結果を得る（最後の出力はsoftmaxのため確率）
    p = np.argmax(y) # 最も確率の高い要素のインデックスを取得
    if p == t[i]: # 最も確率が高いと予測した数字と教師データが同じか確認
        accuracy_cnt += 1 # 同じ場合はカウントを1増やす

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

def mean_squared_error(y, t):
  return 0.5 * np.sum((y - t) ** 2)

y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

mean_squared_error(np.array(y), np.array(t))

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]

mean_squared_error(np.array(y), np.array(t))

def cross_entropy_error(y, t):
  delta = 1e-7 #すごい小さい数
  return -np.sum(t * np.log(y + delta))

y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

cross_entropy_error(np.array(y), np.array(t))

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]

cross_entropy_error(np.array(y), np.array(t))

import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)
print(x_train.shape)
print(t_train.shape)

train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

np.random.choice(60000, 10)

def cross_entropy_error(y, t):
  if y.ndim == 1:
    t = t.reshape(1, t.size)
    y = y.reshape(1, y.size)

  batch_size = y.shape[0]
  return -np.sum(t * np.log(y + 1e-7)) / batch_size

def cross_entropy_error(y, t):
  if y.ndim == 1:
    t = t.reshape(1, t.size)
    y = y.reshape(1, y.size)

  # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換
  if t.size == y.size:
    t = t.argmax(axis=1)

  batch_size = y.shape[0]
  return -np.sum(np.log(y[np.arrange(batch_size), t] + 1e-7)) / batch_size

def function_1(x):
  return x[0] ** 2 + x[1] ** 2

def numerical_gradient(f, x):
  h = 1e-7 # 0.0001
  grad = np.zeros_like(x) # xと同じ形状の配列を生成

  for idx in range(x.size):
    tmp_val = x[idx]
    # f(x+h)の計算
    x[idx] = tmp_val + h
    fxh1 = f(x)

    # f(x-h)の計算
    x[idx] = tmp_val - h
    fxh2 = f(x)

    grad[idx] = (fxh1 - fxh2) / (2 * h)
    x[idx] = tmp_val # 値を元に戻す

  return grad

numerical_gradient(function_1, np.array([3.0, 4.0]))

numerical_gradient(function_1, np.array([3.0, 0.0]))

# coding: utf-8
# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3
import numpy as np
import matplotlib.pylab as plt
from mpl_toolkits.mplot3d import Axes3D

def _numerical_gradient_no_batch(f, x):
    h = 1e-4  # 0.0001
    grad = np.zeros_like(x)

    for idx in range(x.size):
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + h
        fxh1 = f(x)  # f(x+h)

        x[idx] = tmp_val - h
        fxh2 = f(x)  # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2*h)

        x[idx] = tmp_val  # 値を元に戻す

    return grad


def numerical_gradient(f, X):
    if X.ndim == 1:
        return _numerical_gradient_no_batch(f, X)
    else:
        grad = np.zeros_like(X)

        for idx, x in enumerate(X):
            grad[idx] = _numerical_gradient_no_batch(f, x)

        return grad


def function_2(x):
    if x.ndim == 1:
        return np.sum(x**2)
    else:
        return np.sum(x**2, axis=1)


def tangent_line(f, x):
    d = numerical_gradient(f, x)
    print(d)
    y = f(x) - d*x
    return lambda t: d*t + y


if __name__ == '__main__':
    x0 = np.arange(-2, 2.5, 0.25)
    x1 = np.arange(-2, 2.5, 0.25)
    X, Y = np.meshgrid(x0, x1)

    X = X.flatten()
    Y = Y.flatten()

    grad = numerical_gradient(function_2, np.array([X, Y]).T).T

    plt.figure()
    plt.quiver(X, Y, -grad[0], -grad[1],  angles="xy",color="#666666")
    plt.xlim([-2, 2])
    plt.ylim([-2, 2])
    plt.xlabel('x0')
    plt.ylabel('x1')
    plt.grid()
    plt.draw()
    plt.show()

from common.gradient import numerical_gradient
from common.functions import *

class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # この関数はこのクラスが呼び出された（インスタンス化）ときに実行される
        # 重みを入れる変数を用意する
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self, x):
        # 現在の重みパラメータを使って文字認識する
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']

        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)

        return y

    # x:入力データ, t:教師データ
    def loss(self, x, t):
        # 損失関数を利用してlossを出す
        y = self.predict(x)

        return cross_entropy_error(y, t)

    def accuracy(self, x, t):
        # 正答率を出す
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        # 微分を使って勾配を出す
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        return grads

    def gradient(self, x, t):
        # 誤差伝播法を使って勾配を出す
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}

        batch_num = x.shape[0]

        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)

        # backward
        dy = (y - t) / batch_num
        grads['W2'] = np.dot(z1.T, dy)
        grads['b2'] = np.sum(dy, axis=0)

        dz1 = np.dot(dy, W2.T)
        da1 = sigmoid_grad(a1) * dz1
        grads['W1'] = np.dot(x.T, da1)
        grads['b1'] = np.sum(da1, axis=0)

        return grads

(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)
print(x_train.shape)
print(t_train.shape)

train_size = x_train.shape[0]

# ハイパーパラメータ
iters_num = 1000000000  # 繰り返しの回数（バッチ数）を適宜設定する
batch_size = 1000
learning_rate = 0.1

# 損失と認識率の保管先
train_loss_list = []
train_acc_list = []
test_acc_list = []

# クラスからインスタンスを作成
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 何バッチで何エポックか出す
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # ミニバッチ
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 勾配の計算
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)

    # パラメータの更新
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    # 損失を出す
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    # エポックごとに認識率を記録
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        if train_acc >= 0.95:
          print("train acc OK, test acc | " + str(train_acc) + ", " + str(test_acc))
        elif test_acc >= 0.95:
          print("train acc , test acc OK | " + str(train_acc) + ", " + str(test_acc))
        elif test_acc >= 0.95 and train_acc >= 0.95:
          print("train acc OK, test acc OK | " + str(train_acc) + ", " + str(test_acc))
        else:
          print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))
        if test_acc >= 0.95 and train_acc >= 0.95:
          break
print("学習終了")

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt

def showImg(x):
    example = x.reshape((28, 28))
    plt.figure()
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(example)
    plt.show()
    return

test_size = 20
test_mask = np.random.choice(10000, test_size)
x = x_test[test_mask]
t = t_test[test_mask]
tmp = "0"
Ans = 0
persent = 0

for i in range(20):
    y = network.predict(x[i])
    p= np.argmax(y)

    if str(t[i]) == "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]":
      tmp = "0"
    if str(t[i]) == "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]":
      tmp = "1"
    if str(t[i]) == "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]":
      tmp = "2"
    if str(t[i]) == "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]":
      tmp = "3"
    if str(t[i]) == "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]":
      tmp = "4"
    if str(t[i]) == "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]":
      tmp = "5"
    if str(t[i]) == "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]":
      tmp = "6"
    if str(t[i]) == "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]":
      tmp = "7"
    if str(t[i]) == "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]":
      tmp = "8"
    if str(t[i]) == "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]":
      tmp = "9"
    print("正解 ", tmp)
    print("判定[ " + str(p) + " ]")
    if tmp == str(p):
      Ans = Ans + 1
    count = 0
    for v in y:
        print("["+str(count)+"] {:.2%}".format(v))
        count += 1
    showImg(x[i])
    tmp = "0"
persent = Ans / 20 * 100
print("正解率" + str(persent) + "%")